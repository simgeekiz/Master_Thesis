{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import jsonlines\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import xml.etree.ElementTree as ET\n",
    "from subprocess import Popen, PIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_folder='./../Master_Thesis' \n",
    "shuffle_num=1\n",
    "\n",
    "data_path = os.path.join(project_folder, 'Data/thesis_data_shuffle-' + str(shuffle_num) + '_.json')\n",
    "with open(data_path) as reader:\n",
    "    data_ = json.load(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_['metadata']['enrichments'] = {\n",
    "    'tokenization': {\n",
    "        'tool': 'spaCy',\n",
    "        'version': spacy.__version__,\n",
    "        'key': 'token.text'        \n",
    "    },\n",
    "    'part-of-speech': {\n",
    "        'tool': 'spaCy',\n",
    "        'version': spacy.__version__,\n",
    "        'key': 'token.tag_'\n",
    "    },\n",
    "    'named-entities': {\n",
    "        'tool': 'spaCy',\n",
    "        'version': spacy.__version__,\n",
    "        'key': 'token.ent_type_'\n",
    "    },\n",
    "     'Heidel-Time': {\n",
    "        'tool': 'Heidel-Time-Tagger',\n",
    "        'version': 'unknown'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrepareHeidelTime(sentence, tokens, input_lang='english', TMP_FOLDER_PATH='/tmp/'):\n",
    "    txt_file_path = os.path.join(TMP_FOLDER_PATH,\n",
    "                             'time_enrich_tmp_{}.txt'.format(str(randint(10000, 99999))))\n",
    "    \n",
    "    with open(txt_file_path, 'w') as txt_file:\n",
    "    #     for sentence in self.doc.sentences():\n",
    "        txt_file.write(sentence+ \"\\n\")\n",
    "\n",
    "    popen_job = Popen(['java', '-jar',\n",
    "                       'de.unihd.dbs.heideltime.standalone.jar',\n",
    "                       txt_file_path, '-t', 'NEWS', '-l', input_lang, '-pos', 'no'],\n",
    "                       stdout=PIPE,\n",
    "                       cwd='/kuacc/users/simgebasar/heideltime-standalone')\n",
    "    popen_job.wait()\n",
    "\n",
    "    os.remove(txt_file_path)\n",
    "    # Skip the first three lines of the document which is XML definition\n",
    "    start_lines = ''\n",
    "    for _ in range(3):\n",
    "        line = popen_job.stdout.readline()\n",
    "        line = str(line, 'utf-8')\n",
    "        start_lines += line\n",
    "\n",
    "    line = popen_job.stdout.readline()\n",
    "    line = str(line, 'utf-8')\n",
    "\n",
    "\n",
    "    # for sentence in self.doc.sentences():\n",
    "    if '<TIMEX' in line:\n",
    "        line = start_lines + line + '</TimeML>'\n",
    "        try:\n",
    "            time_expr_xml = ET.fromstring(line)\n",
    "        except ET.ParseError as exp:\n",
    "            time_expr_xml = None\n",
    "            print(\"Error while reading TimeML. Time tags may not occur \" \\\n",
    "                           \"in the sentence. Traceback: %s\", str(exp))\n",
    "            pass # continue\n",
    "\n",
    "    if time_expr_xml:\n",
    "        times = [(time, timetime.attrib['type']) for timetime in time_expr_xml for time in timetime.text.split(' ')]\n",
    "    else:\n",
    "        times = []\n",
    "    time_exprs = []\n",
    "    for word in tokens:\n",
    "        if times and word == times[0][0]:\n",
    "            time_exprs.append((word, times[0][1]))\n",
    "            times.pop(0)\n",
    "        else:\n",
    "            time_exprs.append((word,'0'))\n",
    "            \n",
    "    return time_exprs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'article_nums': {'test': 32, 'train': 251, 'valid': 32},\n",
      " 'enrichments': {'Heidel-Time': {'tool': 'Heidel-Time-Tagger',\n",
      "                                 'version': 'unknown'},\n",
      "                 'named-entities': {'key': 'token.ent_type_',\n",
      "                                    'tool': 'spaCy',\n",
      "                                    'version': '2.0.12'},\n",
      "                 'part-of-speech': {'key': 'token.tag_',\n",
      "                                    'tool': 'spaCy',\n",
      "                                    'version': '2.0.12'},\n",
      "                 'tokenization': {'key': 'token.text',\n",
      "                                  'tool': 'spaCy',\n",
      "                                  'version': '2.0.12'}},\n",
      " 'label_nums': {'test': {'0': 320, '1': 116, '2': 5},\n",
      "                'train': {'0': 2472, '1': 1053, '2': 57},\n",
      "                'valid': {'0': 268, '1': 130, '2': 1}},\n",
      " 'raw_data_path': '/home/aorus/workspaces/simge/corpus/Sentence/Newindianexpress/20181001-newindianexpress_sentence_classification_adjudicated_20181218.json',\n",
      " 'sentence_nums': {'test': 441, 'train': 3582, 'valid': 399},\n",
      " 'split_ratio': {'test': 10, 'train': 80, 'valid': 10}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(data_['metadata'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'time_expr_xml' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-285082a6a681>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m             article['sentences'][sent_ind]['ner'] = [token.ent_type_ if token.ent_iob_ != 'O' else 'O' \n\u001b[1;32m     11\u001b[0m                                                      for token in doc]\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mheidel_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPrepareHeidelTime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0marticle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msent_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'heidel'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword_tag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheidel_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-a6888402f8cf>\u001b[0m in \u001b[0;36mPrepareHeidelTime\u001b[0;34m(sentence, tokens, input_lang, TMP_FOLDER_PATH)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mpass\u001b[0m \u001b[0;31m# continue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtime_expr_xml\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mtimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrib\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtimetime\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtime_expr_xml\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtimetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'time_expr_xml' referenced before assignment"
     ]
    }
   ],
   "source": [
    "for key, articles in data_['data'].items():\n",
    "    for article in articles:\n",
    "        for sent_ind, sentence in enumerate(article['sentences']):\n",
    "            \n",
    "            doc = nlp(sentence['sentence'])\n",
    "            \n",
    "            tokens = [token.text for token in doc]\n",
    "            article['sentences'][sent_ind]['tokens'] = tokens \n",
    "            article['sentences'][sent_ind]['pos'] = [token.tag_ for token in doc] \n",
    "            article['sentences'][sent_ind]['ner'] = [token.ent_type_ if token.ent_iob_ != 'O' else 'O' \n",
    "                                                     for token in doc]\n",
    "            heidel_list = PrepareHeidelTime(sentence['sentence'], tokens)\n",
    "            article['sentences'][sent_ind]['heidel'] = [word_tag[1] for word_tag in heidel_list]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/thesis_data_shuffle-' + str(shuffle_num) + '_enriched_.json', 'w') as outfile:\n",
    "    json.dump(data_, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wikipedia2vec import Wikipedia2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FILE='Data/enwiki_20180420_500d.pkl.bz2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki2vec = Wikipedia2Vec.load(MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHECK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "opt_results_path = 'Scikit-Experiments/Results/results_070919_tfidf_only_mindf_001_maxdf_0_12_numberstoplwords.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(opt_results_path, 'rb') as file_:\n",
    "    opt_results = pickle.load(file_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_results.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heidel Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "heidel_list = PrepareHeidelTime(article['sentences'][1]['sentence'], article['sentences'][1]['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
