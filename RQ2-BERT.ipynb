{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "from keras.models import Model, Input\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Bidirectional, Lambda\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.callbacks import PlotCurves\n",
    "from src.custom_functions import f1_macro, f1_micro \n",
    "from src.load_data import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data, metadata = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 512\n",
    "if max_seq_length > 512:\n",
    "    print('!!!!!!! WARNING: BERT does not accept lenght > 512')\n",
    "    max_seq_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data_, max_seq_length, bert_path, to_categorize):\n",
    "    \n",
    "    tokenizer = create_tokenizer_from_hub_module(bert_path)\n",
    "    \n",
    "    # !!! For BERT input, each sentence should be in an array\n",
    "    X = np.array([[\" \".join(sentence['sentence'].replace('\\n', '').strip().split()[0:max_seq_length])]\n",
    "                  for article in data_ \n",
    "                  for sentence in article['sentences']], dtype=object)\n",
    "\n",
    "    y = [sentence['label'] \n",
    "                  for article in data_\n",
    "                  for sentence in article['sentences']]\n",
    "    \n",
    "    examples_ = convert_text_to_examples(X, y)\n",
    "    \n",
    "    (input_ids, input_masks, segment_ids, labels_) = \\\n",
    "            convert_examples_to_features(tokenizer, examples_, max_seq_length=max_seq_length)\n",
    "    \n",
    "    if to_categorize:\n",
    "        labels_ = to_categorical(labels_)\n",
    "    \n",
    "    return [input_ids, input_masks, segment_ids], labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0901 23:56:44.645421 140486611460224 deprecation_wrapper.py:119] From /home/aorus/workspaces/simge/Master_Thesis/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "Converting examples to features: 100%|██████████| 3582/3582 [00:01<00:00, 2470.84it/s]\n",
      "Converting examples to features: 100%|██████████| 399/399 [00:00<00:00, 2543.04it/s]\n",
      "Converting examples to features: 100%|██████████| 441/441 [00:00<00:00, 2548.65it/s]\n"
     ]
    }
   ],
   "source": [
    "X_tra, y_tra = split_data(train_data, max_seq_length, bert_path, True)\n",
    "X_val, y_val = split_data(valid_data, max_seq_length, bert_path, True)\n",
    "X_test, y_test = split_data(test_data, max_seq_length, bert_path, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data_, max_len, n_tags, is_test=False):\n",
    "    \n",
    "    X = []\n",
    "    for article in data_:\n",
    "        new_seq = []\n",
    "        for i in range(max_len):\n",
    "            try:\n",
    "                new_seq.append(article['sentences'][i]['sentence'])\n",
    "            except:\n",
    "                new_seq.append(\"ENDPAD\")\n",
    "        X.append(new_seq)\n",
    "    X = np.array(X)\n",
    "    \n",
    "    if not is_test: \n",
    "        y = [[sent['label'] for sent in article['sentences']] for article in data_]\n",
    "        y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=0)\n",
    "        y = np.array([[to_categorical(y, num_classes=n_tags) for y in sent] for sent in y])\n",
    "    else:\n",
    "        y = np.array([sent['label'] for article in data_ for sent in article['sentences']])\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tra, y_tra = split_data(train_data, max_len, n_tags, False)\n",
    "X_val, y_val = split_data(valid_data, max_len, n_tags, False)\n",
    "X_test, y_test = split_data(test_data, max_len, n_tags, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((251, 60, 2), (32, 60, 2), (441,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tra.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
